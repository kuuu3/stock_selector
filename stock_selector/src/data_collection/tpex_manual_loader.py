"""
æ«ƒè²·ä¸­å¿ƒæ‰‹å‹•ä¸‹è¼‰æ•¸æ“šè™•ç†æ¨¡çµ„

è™•ç†å¾ https://www.tpex.org.tw/zh-tw/mainboard/trading/info/stock-pricing.html 
æ‰‹å‹•ä¸‹è¼‰çš„ CSV æª”æ¡ˆ

ä½¿ç”¨æ–¹å¼ï¼š
1. å‰å¾€æ«ƒè²·ä¸­å¿ƒå€‹è‚¡æ—¥æˆäº¤è³‡è¨Šé é¢
2. è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚ï¼š3260ï¼‰
3. é¸æ“‡è³‡æ–™å¹´æœˆç¯„åœ
4. é»æ“Šã€Œä¸‹è¼‰CSVæª”(UTF-8)ã€
5. å°‡ä¸‹è¼‰çš„æª”æ¡ˆæ”¾å…¥ data/manual_tpex/ è³‡æ–™å¤¾
6. é‹è¡Œæ­¤æ¨¡çµ„è™•ç†æ•¸æ“š
"""

import pandas as pd
import os
from pathlib import Path
from datetime import datetime
import logging
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

class TPEXManualLoader:
    """è™•ç†æ«ƒè²·ä¸­å¿ƒæ‰‹å‹•ä¸‹è¼‰çš„æ­·å²æ•¸æ“š"""
    
    def __init__(self, manual_data_dir: str = "data/manual_tpex"):
        """
        åˆå§‹åŒ–
        
        Args:
            manual_data_dir: æ‰‹å‹•ä¸‹è¼‰æ•¸æ“šå­˜æ”¾ç›®éŒ„
        """
        self.manual_data_dir = Path(manual_data_dir)
        self.manual_data_dir.mkdir(parents=True, exist_ok=True)
        
    def load_manual_csv(self, csv_file: str) -> pd.DataFrame:
        """
        è¼‰å…¥æ‰‹å‹•ä¸‹è¼‰çš„ CSV æª”æ¡ˆ
        
        Args:
            csv_file: CSV æª”æ¡ˆè·¯å¾‘æˆ–æª”æ¡ˆå
            
        Returns:
            æ¨™æº–åŒ–çš„ DataFrame
        """
        csv_path = Path(csv_file)
        if not csv_path.is_absolute():
            if csv_path.parent.name == 'manual_tpex':
                # å¦‚æœå·²ç¶“æ˜¯ç›¸å°è·¯å¾‘ï¼Œç›´æ¥ä½¿ç”¨
                csv_path = self.manual_data_dir / csv_path.name
            else:
                # å¦‚æœæ˜¯æª”æ¡ˆåï¼ŒåŠ ä¸Šç›®éŒ„
                csv_path = self.manual_data_dir / csv_path
            
        if not csv_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {csv_path}")
            
        logger.info(f"è¼‰å…¥æ‰‹å‹•ä¸‹è¼‰çš„ CSV æª”æ¡ˆ: {csv_path}")
        
        try:
            # æ«ƒè²·ä¸­å¿ƒ CSV æ ¼å¼éœ€è¦è·³éå‰ 4 è¡Œæ¨™é¡Œ
            # å˜—è©¦ä¸åŒçš„ç·¨ç¢¼æ–¹å¼
            for encoding in ['utf-8', 'big5', 'cp950']:
                try:
                    df = pd.read_csv(csv_path, encoding=encoding, skiprows=4)
                    logger.info(f"ä½¿ç”¨ç·¨ç¢¼ {encoding} æˆåŠŸè¼‰å…¥ï¼Œè·³éå‰ 4 è¡Œæ¨™é¡Œ")
                    break
                except UnicodeDecodeError:
                    continue
                except Exception as e:
                    logger.warning(f"ç·¨ç¢¼ {encoding} å¤±æ•—: {e}")
                    continue
            else:
                raise ValueError("ç„¡æ³•è§£æ CSV æª”æ¡ˆç·¨ç¢¼")
                
            # æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼
            df = self._standardize_data(df, csv_path)
            logger.info(f"æˆåŠŸè¼‰å…¥ {len(df)} ç­†æ•¸æ“š")
            
            return df
            
        except Exception as e:
            logger.error(f"è¼‰å…¥ CSV æª”æ¡ˆå¤±æ•—: {e}")
            raise
            
    def _standardize_data(self, df: pd.DataFrame, csv_path: str = None) -> pd.DataFrame:
        """
        æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼
        
        Args:
            df: åŸå§‹ DataFrame
            
        Returns:
            æ¨™æº–åŒ–çš„ DataFrame
        """
        logger.info("é–‹å§‹æ¨™æº–åŒ–æ•¸æ“šæ ¼å¼...")
        
        # é¡¯ç¤ºåŸå§‹æ¬„ä½åç¨±
        logger.info(f"åŸå§‹æ¬„ä½: {list(df.columns)}")
        
        # æ ¹æ“šæ«ƒè²·ä¸­å¿ƒå¯¦éš› CSV æ ¼å¼é€²è¡Œæ¬„ä½æ˜ å°„
        column_mapping = {
            'æ—¥ æœŸ': 'date',
            'é–‹ç›¤': 'open',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æ”¶ç›¤': 'close',
            'æˆäº¤ä»Ÿè‚¡': 'volume',
            'æˆäº¤å¼µæ•¸': 'volume',  # 2025å¹´å¾Œæ”¹ç‚ºæˆäº¤å¼µæ•¸
            'æˆäº¤ä»Ÿå…ƒ': 'transaction_amount',
            'ç­†æ•¸': 'transaction_count',
            'æ¼²è·Œ': 'price_change'
        }
        
        # é‡å‘½åæ¬„ä½
        df = df.rename(columns=column_mapping)
        
        # æå–è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¾æª”æ¡ˆåæå–ï¼‰
        if csv_path:
            stock_code = self._extract_stock_code_from_filename(str(csv_path))
            if stock_code:
                df['stock_code'] = int(stock_code)
                logger.info(f"å¾æª”æ¡ˆåæå–è‚¡ç¥¨ä»£ç¢¼: {stock_code}")
            else:
                logger.warning("ç„¡æ³•å¾æª”æ¡ˆåæå–è‚¡ç¥¨ä»£ç¢¼")
                # å˜—è©¦å¾æª”æ¡ˆåä¸­çš„æ•¸å­—éƒ¨åˆ†æå–
                import re
                match = re.search(r'(\d{4})', str(csv_path))
                if match:
                    df['stock_code'] = int(match.group(1))
                    logger.info(f"ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æå–è‚¡ç¥¨ä»£ç¢¼: {match.group(1)}")
                else:
                    raise ValueError("ç„¡æ³•å¾æª”æ¡ˆåæå–è‚¡ç¥¨ä»£ç¢¼")
        else:
            raise ValueError("æ²’æœ‰æä¾›æª”æ¡ˆè·¯å¾‘ï¼Œç„¡æ³•æå–è‚¡ç¥¨ä»£ç¢¼")
        
        # è½‰æ›æ—¥æœŸæ ¼å¼ï¼ˆæ«ƒè²·ä¸­å¿ƒä½¿ç”¨æ°‘åœ‹å¹´æ ¼å¼ï¼š113/05/08ï¼‰
        if 'date' in df.columns:
            def convert_roc_date(date_str):
                """è½‰æ›æ°‘åœ‹å¹´æ—¥æœŸæ ¼å¼"""
                if pd.isna(date_str) or date_str == '':
                    return None
                try:
                    # æ°‘åœ‹å¹´æ ¼å¼ï¼š113/05/08 -> 2024/05/08
                    parts = str(date_str).split('/')
                    if len(parts) == 3:
                        roc_year = int(parts[0])
                        gregorian_year = roc_year + 1911
                        return f"{gregorian_year}/{parts[1]}/{parts[2]}"
                    return date_str
                except:
                    return date_str
            
            df['date'] = df['date'].apply(convert_roc_date)
            df['date'] = pd.to_datetime(df['date'], format='%Y/%m/%d', errors='coerce')
            df['date'] = df['date'].dt.date
            
        # è½‰æ›æ•¸å€¼æ¬„ä½
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'transaction_amount', 'transaction_count']
        for col in numeric_columns:
            if col in df.columns:
                # ç§»é™¤é€—è™Ÿä¸¦è½‰æ›ç‚ºæ•¸å€¼
                df[col] = df[col].astype(str).str.replace(',', '').str.replace('--', '0')
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                
                # æˆäº¤é‡è½‰æ›
                if col == 'volume':
                    # å¦‚æœæ˜¯æˆäº¤å¼µæ•¸ï¼ˆ2025å¹´å¾Œï¼‰ï¼Œéœ€è¦ä¹˜ä»¥1000è½‰æ›ç‚ºè‚¡
                    # å¦‚æœæ˜¯æˆäº¤ä»Ÿè‚¡ï¼ˆ2024å¹´ï¼‰ï¼Œå·²ç¶“æ˜¯ä»Ÿè‚¡å–®ä½ï¼Œéœ€è¦ä¹˜ä»¥1000è½‰æ›ç‚ºè‚¡
                    df[col] = df[col] * 1000
                
        # ç¢ºä¿è‚¡ç¥¨ä»£ç¢¼ç‚ºå­—ç¬¦ä¸²æ ¼å¼
        if 'stock_code' in df.columns:
            df['stock_code'] = df['stock_code'].astype(str).str.zfill(4)
            
        # é¸æ“‡éœ€è¦çš„æ¬„ä½
        required_columns = ['date', 'stock_code', 'open', 'high', 'low', 'close', 'volume']
        available_columns = [col for col in required_columns if col in df.columns]
        df = df[available_columns]
        
        # æ’åºä¸¦é‡ç½®ç´¢å¼•
        df = df.sort_values(['stock_code', 'date']).reset_index(drop=True)
        
        logger.info(f"æ¨™æº–åŒ–å®Œæˆï¼Œæœ€çµ‚æ¬„ä½: {list(df.columns)}")
        return df
        
    def process_all_manual_files(self) -> pd.DataFrame:
        """
        è™•ç†æ‰€æœ‰æ‰‹å‹•ä¸‹è¼‰çš„ CSV æª”æ¡ˆ
        
        Returns:
            åˆä½µå¾Œçš„ DataFrame
        """
        logger.info("é–‹å§‹è™•ç†æ‰€æœ‰æ‰‹å‹•ä¸‹è¼‰çš„æª”æ¡ˆ...")
        
        csv_files = list(self.manual_data_dir.glob("*.csv"))
        if not csv_files:
            logger.warning(f"åœ¨ {self.manual_data_dir} ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
            return pd.DataFrame()
            
        all_data = []
        for csv_file in csv_files:
            try:
                logger.info(f"è™•ç†æª”æ¡ˆ: {csv_file.name}")
                df = self.load_manual_csv(csv_file)
                
                # å¾æª”æ¡ˆåæå–è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚æœæ•¸æ“šä¸­æ²’æœ‰ï¼‰
                if 'stock_code' not in df.columns:
                    stock_code = self._extract_stock_code_from_filename(csv_file.name)
                    if stock_code:
                        df['stock_code'] = stock_code
                    else:
                        logger.warning(f"ç„¡æ³•å¾æª”æ¡ˆå {csv_file.name} æå–è‚¡ç¥¨ä»£ç¢¼")
                        continue
                        
                all_data.append(df)
                
            except Exception as e:
                logger.error(f"è™•ç†æª”æ¡ˆ {csv_file.name} å¤±æ•—: {e}")
                continue
                
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df = combined_df.drop_duplicates(subset=['stock_code', 'date']).sort_values(['stock_code', 'date'])
            logger.info(f"æˆåŠŸè™•ç† {len(all_data)} å€‹æª”æ¡ˆï¼Œåˆè¨ˆ {len(combined_df)} ç­†æ•¸æ“š")
            return combined_df
        else:
            logger.error("æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æª”æ¡ˆ")
            return pd.DataFrame()
            
    def _extract_stock_code_from_filename(self, filename: str) -> Optional[str]:
        """
        å¾æª”æ¡ˆåæå–è‚¡ç¥¨ä»£ç¢¼
        
        Args:
            filename: æª”æ¡ˆå
            
        Returns:
            è‚¡ç¥¨ä»£ç¢¼æˆ– None
        """
        import re
        
        # å˜—è©¦æå– 4 ä½æ•¸å­—
        match = re.search(r'(\d{4})', filename)
        if match:
            return match.group(1)
            
        return None
        
    def save_processed_data(self, df: pd.DataFrame, output_file: str = "data/processed/tpex_historical.csv"):
        """
        ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
        
        Args:
            df: è™•ç†å¾Œçš„ DataFrame
            output_file: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        df.to_csv(output_path, index=False)
        logger.info(f"å·²ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“šåˆ°: {output_path}")
        
    def get_available_files(self) -> List[str]:
        """
        ç²å–å¯ç”¨çš„æ‰‹å‹•ä¸‹è¼‰æª”æ¡ˆåˆ—è¡¨
        
        Returns:
            æª”æ¡ˆååˆ—è¡¨
        """
        csv_files = list(self.manual_data_dir.glob("*.csv"))
        return [f.name for f in csv_files]


def main():
    """ä¸»å‡½æ•¸ - è™•ç†æ‰‹å‹•ä¸‹è¼‰çš„æ•¸æ“š"""
    logging.basicConfig(level=logging.INFO)
    
    loader = TPEXManualLoader()
    
    # æª¢æŸ¥å¯ç”¨æª”æ¡ˆ
    available_files = loader.get_available_files()
    if not available_files:
        print("âŒ åœ¨ data/manual_tpex/ ä¸­æ²’æœ‰æ‰¾åˆ° CSV æª”æ¡ˆ")
        print("\nğŸ“‹ ä½¿ç”¨èªªæ˜ï¼š")
        print("1. å‰å¾€æ«ƒè²·ä¸­å¿ƒå€‹è‚¡æ—¥æˆäº¤è³‡è¨Šé é¢")
        print("   https://www.tpex.org.tw/zh-tw/mainboard/trading/info/stock-pricing.html")
        print("2. è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼ï¼ˆå¦‚ï¼š3260ï¼‰")
        print("3. é¸æ“‡è³‡æ–™å¹´æœˆç¯„åœ")
        print("4. é»æ“Šã€Œä¸‹è¼‰CSVæª”(UTF-8)ã€")
        print("5. å°‡ä¸‹è¼‰çš„æª”æ¡ˆæ”¾å…¥ data/manual_tpex/ è³‡æ–™å¤¾")
        print("6. é‡æ–°é‹è¡Œæ­¤è…³æœ¬")
        return
        
    print(f"âœ… æ‰¾åˆ° {len(available_files)} å€‹ CSV æª”æ¡ˆ:")
    for file in available_files:
        print(f"  - {file}")
        
    # è™•ç†æ‰€æœ‰æª”æ¡ˆ
    print("\nğŸ”„ é–‹å§‹è™•ç†æª”æ¡ˆ...")
    combined_df = loader.process_all_manual_files()
    
    if not combined_df.empty:
        print(f"âœ… æˆåŠŸè™•ç† {len(combined_df)} ç­†æ•¸æ“š")
        print(f"ğŸ“Š åŒ…å«è‚¡ç¥¨: {sorted(combined_df['stock_code'].unique())}")
        
        # ä¿å­˜è™•ç†å¾Œçš„æ•¸æ“š
        loader.save_processed_data(combined_df)
        
        # é¡¯ç¤ºçµ±è¨ˆä¿¡æ¯
        print("\nğŸ“ˆ æ•¸æ“šçµ±è¨ˆ:")
        for stock_code in sorted(combined_df['stock_code'].unique()):
            stock_data = combined_df[combined_df['stock_code'] == stock_code]
            print(f"  è‚¡ç¥¨ {stock_code}: {len(stock_data)} ç­†æ•¸æ“š")
            if len(stock_data) > 0:
                date_range = f"{stock_data['date'].min()} åˆ° {stock_data['date'].max()}"
                print(f"    æ—¥æœŸç¯„åœ: {date_range}")
    else:
        print("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æ•¸æ“š")


if __name__ == "__main__":
    main()
