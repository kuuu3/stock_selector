"""
çµ±ä¸€æ•¸æ“šæº–å‚™è…³æœ¬
æ•´åˆæ•¸æ“šæŠ“å–å’Œæ–°èè™•ç†åŠŸèƒ½
"""

import sys
from pathlib import Path
import logging
import pandas as pd
from datetime import datetime, timedelta
import argparse
import time

# æ·»åŠ  src ç›®éŒ„åˆ° Python è·¯å¾‘
src_path = Path(__file__).parent / "src"
sys.path.insert(0, str(src_path))

from src.data_collection import PriceFetcher
from src.data_collection.api_fetcher import APIFetcher
from src.data_collection.news_scraper import NewsScraper
from src.preprocessing.sentiment_analyzer import SentimentAnalyzer
from src.config import RAW_PRICES_FILE, RAW_NEWS_FILE, get_data_file_path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def extract_stock_codes(text: str) -> list:
    """å¾æ–‡æœ¬ä¸­æå–è‚¡ç¥¨ä»£ç¢¼"""
    import re
    # è‚¡ç¥¨ä»£ç¢¼æ¨¡å¼ï¼ˆ4ä½æ•¸å­—ï¼‰
    stock_pattern = r'\\b(\\d{4})\\b'
    matches = re.findall(stock_pattern, text)
    
    # éæ¿¾å‡ºå¯èƒ½çš„è‚¡ç¥¨ä»£ç¢¼ï¼ˆå°è‚¡ä»£ç¢¼é€šå¸¸åœ¨1000-9999ä¹‹é–“ï¼‰
    stock_codes = []
    for match in matches:
        if 1000 <= int(match) <= 9999:
            stock_codes.append(match)
    
    return list(set(stock_codes))  # å»é‡


def analyze_stock_sentiments(sentiment_df: pd.DataFrame) -> dict:
    """åˆ†æå„è‚¡ç¥¨çš„æƒ…æ„Ÿ"""
    stock_sentiments = {}
    
    for _, row in sentiment_df.iterrows():
        stock_codes_str = row.get('stock_codes', '')
        if not stock_codes_str:
            continue
        
        stock_codes = stock_codes_str.split('|') if stock_codes_str else []
        
        for stock_code in stock_codes:
            if stock_code not in stock_sentiments:
                stock_sentiments[stock_code] = {
                    'news_count': 0,
                    'sentiment_distribution': {'positive': 0, 'negative': 0, 'neutral': 0},
                    'total_score': 0.0,
                    'total_confidence': 0.0
                }
            
            # æ›´æ–°çµ±è¨ˆ
            sentiment_info = stock_sentiments[stock_code]
            sentiment_info['news_count'] += 1
            sentiment_info['sentiment_distribution'][row['sentiment']] += 1
            sentiment_info['total_score'] += row['sentiment_score']
            sentiment_info['total_confidence'] += row['confidence']
    
    # è¨ˆç®—å¹³å‡å€¼å’Œä¸»è¦æƒ…æ„Ÿ
    for stock_code, sentiment_info in stock_sentiments.items():
        news_count = sentiment_info['news_count']
        sentiment_info['avg_score'] = sentiment_info['total_score'] / news_count
        sentiment_info['avg_confidence'] = sentiment_info['total_confidence'] / news_count
        
        # ç¢ºå®šä¸»è¦æƒ…æ„Ÿ
        sentiment_dist = sentiment_info['sentiment_distribution']
        dominant_sentiment = max(sentiment_dist, key=sentiment_dist.get)
        sentiment_info['dominant_sentiment'] = dominant_sentiment
    
    return stock_sentiments


def check_price_data():
    """æª¢æŸ¥ç¾æœ‰è‚¡åƒ¹æ•¸æ“š"""
    price_file = RAW_PRICES_FILE
    if not price_file.exists():
        logger.info("æ²’æœ‰æ‰¾åˆ°ç¾æœ‰è‚¡åƒ¹æ•¸æ“šæ–‡ä»¶")
        return None, None
    try:
        df = pd.read_csv(price_file)
        df['date'] = pd.to_datetime(df['date'])
        df['stock_code'] = df['stock_code'].astype(str)
        latest_date = df['date'].max()
        data_count = len(df)
        stock_count = df['stock_code'].nunique()
        days_old = (datetime.now() - latest_date).days
        logger.info(f"ç¾æœ‰è‚¡åƒ¹æ•¸æ“š: {data_count} ç­†è¨˜éŒ„, {stock_count} æ”¯è‚¡ç¥¨")
        logger.info(f"æœ€æ–°æ•¸æ“šæ—¥æœŸ: {latest_date.strftime('%Y-%m-%d')}")
        logger.info(f"æ•¸æ“šå¹´é½¡: {days_old} å¤©")
        return df, latest_date
    except Exception as e:
        logger.error(f"è®€å–ç¾æœ‰è‚¡åƒ¹æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None


def fetch_price_data(force_refresh=False, use_api=False):
    """ç²å–è‚¡åƒ¹æ•¸æ“š"""
    logger.info("=== é–‹å§‹ç²å–è‚¡ç¥¨æ•¸æ“š ===")
    existing_df, latest_date = check_price_data()
    
    # æª¢æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±çš„è‚¡ç¥¨ï¼ˆç„¡è«–æ•¸æ“šå¹´é½¡ï¼‰
    if existing_df is not None and not force_refresh:
        from src.config import DATA_COLLECTION_CONFIG
        existing_stocks = set(existing_df['stock_code'].astype(str).unique())
        all_stocks = set(DATA_COLLECTION_CONFIG["STOCK_LIST"])
        missing_stocks = all_stocks - existing_stocks
        
        if missing_stocks:
            logger.info(f"ç™¼ç¾ {len(missing_stocks)} æ”¯è‚¡ç¥¨ç¼ºå¤±æ•¸æ“š: {sorted(missing_stocks)}")
        else:
            days_old = (datetime.now() - latest_date).days
            if days_old <= 1:
                logger.info("è‚¡åƒ¹æ•¸æ“šå·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°")
                return existing_df
            elif days_old <= 3:
                logger.info(f"è‚¡åƒ¹æ•¸æ“šè¼ƒèˆŠ ({days_old} å¤©)ï¼Œå»ºè­°æ›´æ–°")
            else:
                logger.info(f"è‚¡åƒ¹æ•¸æ“šéèˆŠ ({days_old} å¤©)ï¼Œæ­£åœ¨æ›´æ–°...")
    try:
        # é¦–å…ˆå˜—è©¦ä½¿ç”¨ twstockï¼Œå¦‚æœå¤±æ•—å‰‡ä½¿ç”¨ API
        price_fetcher = PriceFetcher()
        api_fetcher = APIFetcher()
        
        if force_refresh:
            logger.info("å¼·åˆ¶åˆ·æ–°æ¨¡å¼ï¼šé‡æ–°ç²å–æ‰€æœ‰è‚¡åƒ¹æ•¸æ“š")
            
            if use_api:
                logger.info("ä½¿ç”¨ API ç²å–æ•¸æ“š...")
                new_df = api_fetcher.fetch_all_stocks(force_refresh=True)
                if not new_df.empty:
                    logger.info("API ç²å–æ•¸æ“šæˆåŠŸ")
                else:
                    logger.error("API ç„¡æ³•ç²å–æ•¸æ“š")
            else:
                # å…ˆå˜—è©¦ twstock
                logger.info("å˜—è©¦ä½¿ç”¨ twstock ç²å–æ•¸æ“š...")
                new_df = price_fetcher.fetch_all_stocks(save_to_file=False)
                
                if new_df.empty or len(new_df['stock_code'].unique()) < len(price_fetcher.stock_list) * 0.8:
                    logger.warning("twstock ç²å–æ•¸æ“šä¸å®Œæ•´ï¼Œå˜—è©¦ä½¿ç”¨ API...")
                    api_df = api_fetcher.fetch_all_stocks(force_refresh=True)
                    if not api_df.empty:
                        new_df = api_df
                        logger.info("ä½¿ç”¨ API æˆåŠŸç²å–æ•¸æ“š")
                    else:
                        logger.error("API ä¹Ÿç„¡æ³•ç²å–æ•¸æ“š")
                else:
                    logger.info("twstock ç²å–æ•¸æ“šæˆåŠŸ")
        else:
            logger.info("å¢é‡æ›´æ–°æ¨¡å¼ï¼šæª¢æŸ¥ç¼ºå¤±çš„è‚¡åƒ¹æ•¸æ“š")
            
            if use_api:
                # ä½¿ç”¨ API é€²è¡Œå¢é‡æ›´æ–°
                new_df = api_fetcher.fetch_missing_stocks(existing_df)
                if new_df.empty:
                    logger.info("æ²’æœ‰æ–°è‚¡åƒ¹æ•¸æ“šéœ€è¦æ›´æ–°")
                    return existing_df
                if existing_df is None or existing_df.empty:
                    logger.info("Cold-startï¼šä½¿ç”¨å®Œæ•´çš„æ–°è‚¡åƒ¹æ•¸æ“š")
                else:
                    new_df = pd.concat([existing_df, new_df], ignore_index=True)
                    new_df = new_df.sort_values(['stock_code', 'date']).reset_index(drop=True)
            else:
                # ä½¿ç”¨ twstock é€²è¡Œå¢é‡æ›´æ–°
                new_data = price_fetcher.fetch_incremental_data(existing_df)
                if new_data.empty:
                    logger.info("æ²’æœ‰æ–°è‚¡åƒ¹æ•¸æ“šéœ€è¦æ›´æ–°")
                    return existing_df
                if existing_df is None or existing_df.empty:
                    logger.info("Cold-startï¼šä½¿ç”¨å®Œæ•´çš„æ–°è‚¡åƒ¹æ•¸æ“š")
                    new_df = new_data
                else:
                    new_df = pd.concat([existing_df, new_data], ignore_index=True)
                    new_df = new_df.sort_values(['stock_code', 'date']).reset_index(drop=True)
            
        # ä¿å­˜æ•¸æ“š
        if not new_df.empty:
            new_df.to_csv(RAW_PRICES_FILE, index=False)
            logger.info(f"è‚¡åƒ¹æ•¸æ“šå·²ä¿å­˜åˆ°: {RAW_PRICES_FILE}")
        if new_df.empty:
            logger.error("ç„¡æ³•ç²å–æ–°è‚¡åƒ¹æ•¸æ“š")
            return None
        new_df['date'] = pd.to_datetime(new_df['date'])
        latest_new = new_df['date'].max()
        logger.info(f"æˆåŠŸç²å–è‚¡åƒ¹æ•¸æ“š")
        logger.info(f"æ•¸æ“šç­†æ•¸: {len(new_df)}")
        logger.info(f"è‚¡ç¥¨æ•¸é‡: {new_df['stock_code'].nunique()}")
        logger.info(f"æœ€æ–°æ—¥æœŸ: {latest_new.strftime('%Y-%m-%d')}")
        return new_df
    except Exception as e:
        logger.error(f"ç²å–è‚¡åƒ¹æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def check_news_data():
    """æª¢æŸ¥ç¾æœ‰æ–°èæ•¸æ“š"""
    news_file = RAW_NEWS_FILE
    if not news_file.exists():
        logger.info("æ²’æœ‰æ‰¾åˆ°ç¾æœ‰æ–°èæ•¸æ“šæ–‡ä»¶")
        return None, None
    try:
        df = pd.read_csv(news_file, encoding='utf-8-sig')
        df['scraped_time'] = pd.to_datetime(df['scraped_time'])
        latest_time = df['scraped_time'].max()
        data_count = len(df)
        days_old = (datetime.now() - latest_time).days
        logger.info(f"ç¾æœ‰æ–°èæ•¸æ“š: {data_count} å‰‡")
        logger.info(f"æœ€æ–°æŠ“å–æ™‚é–“: {latest_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ•¸æ“šå¹´é½¡: {days_old} å¤©")
        return df, latest_time
    except Exception as e:
        logger.warning(f"è®€å–ç¾æœ‰æ–°èæ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None, None


def fetch_news_data(force_refresh=False, pages_per_source=3):
    """ç²å–æ–°èæ•¸æ“š"""
    logger.info("=== é–‹å§‹ç²å–æ–°èæ•¸æ“š ===")
    existing_df, latest_time = check_news_data()
    if existing_df is not None and not force_refresh:
        days_old = (datetime.now() - latest_time).days
        if days_old <= 0:
            logger.info("æ–°èæ•¸æ“šå·²æ˜¯æœ€æ–°ï¼Œç„¡éœ€æ›´æ–°")
            return existing_df
        elif days_old <= 1:
            logger.info(f"æ–°èæ•¸æ“šè¼ƒèˆŠ ({days_old} å¤©)ï¼Œå»ºè­°æ›´æ–°")
        else:
            logger.info(f"æ–°èæ•¸æ“šéèˆŠ ({days_old} å¤©)ï¼Œæ­£åœ¨æ›´æ–°...")
    
    try:
        # ä½¿ç”¨çµ±ä¸€æ–°èçˆ¬èŸ²
        logger.info("ä½¿ç”¨çµ±ä¸€æ–°èçˆ¬èŸ²...")
        scraper = NewsScraper()
        news_list = scraper.scrape_all_news(
            cnyes_limit=15,
            yahoo_limit=15,
            cna_limit=10
        )
        
        if news_list:
            df = pd.DataFrame(news_list)
            df['scraped_time'] = datetime.now()
            logger.info(f"çµ±ä¸€æ–°èçˆ¬èŸ²æˆåŠŸç²å– {len(df)} å‰‡æ–°è")
            return df
        else:
            logger.warning("çµ±ä¸€æ–°èçˆ¬èŸ²æœªç²å–åˆ°ä»»ä½•æ–°è")
            return None
    except Exception as e:
        logger.error(f"ç²å–æ–°èæ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def analyze_news_sentiment(news_df):
    """åˆ†ææ–°èæƒ…æ„Ÿ"""
    logger.info("=== é–‹å§‹æ–°èæƒ…æ„Ÿåˆ†æ ===")
    
    try:
        analyzer = SentimentAnalyzer()
        
        # æ‰¹é‡åˆ†ææƒ…æ„Ÿ
        logger.info("é–‹å§‹æ‰¹é‡æƒ…æ„Ÿåˆ†æ...")
        results = []
        
        for idx, row in news_df.iterrows():
            title = row.get('title', '')
            
            # åˆ†ææƒ…æ„Ÿ
            sentiment_result = analyzer.analyze_sentiment(title, method='hybrid')
            
            # æå–è‚¡ç¥¨ä»£ç¢¼
            stock_codes = extract_stock_codes(title)
            
            # æ·»åŠ åˆ°çµæœ
            results.append({
                'index': idx,
                'title': title,
                'source': row.get('source', ''),
                'sentiment': sentiment_result['sentiment'],
                'sentiment_score': sentiment_result['score'],
                'confidence': sentiment_result['confidence'],
                'positive_words': '|'.join(sentiment_result['positive_words']),
                'negative_words': '|'.join(sentiment_result['negative_words']),
                'stock_codes': '|'.join(stock_codes),
                'analyzed_time': datetime.now()
            })
            
            if (idx + 1) % 100 == 0:
                logger.info(f"å·²è™•ç† {idx + 1}/{len(news_df)} å‰‡æ–°è")
        
        # è½‰æ›ç‚º DataFrame
        sentiment_df = pd.DataFrame(results)
        
        # çµ±è¨ˆçµæœ
        sentiment_counts = sentiment_df['sentiment'].value_counts()
        logger.info(f"æƒ…æ„Ÿåˆ†æå®Œæˆ:")
        logger.info(f"  æ­£é¢: {sentiment_counts.get('positive', 0)} å‰‡")
        logger.info(f"  è² é¢: {sentiment_counts.get('negative', 0)} å‰‡")
        logger.info(f"  ä¸­æ€§: {sentiment_counts.get('neutral', 0)} å‰‡")
        logger.info(f"  å¹³å‡ä¿¡å¿ƒåº¦: {sentiment_df['confidence'].mean():.3f}")
        
        # ä¿å­˜çµæœ
        output_file = get_data_file_path("processed/news_with_sentiment.csv")
        output_file.parent.mkdir(parents=True, exist_ok=True)
        sentiment_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        logger.info(f"æƒ…æ„Ÿåˆ†æçµæœå·²ä¿å­˜åˆ°: {output_file}")
        
        return sentiment_df
        
    except Exception as e:
        logger.error(f"æƒ…æ„Ÿåˆ†ææ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


def analyze_stock_sentiment(sentiment_df):
    """åˆ†æå„è‚¡ç¥¨çš„æƒ…æ„Ÿ"""
    logger.info("=== åˆ†æå„è‚¡ç¥¨çš„æƒ…æ„Ÿ ===")
    
    try:
        stock_sentiments = analyze_stock_sentiments(sentiment_df)
        
        if stock_sentiments:
            # é¡¯ç¤ºå‰10å€‹è‚¡ç¥¨çš„æƒ…æ„Ÿåˆ†æçµæœ
            logger.info("å‰10å€‹è‚¡ç¥¨çš„æƒ…æ„Ÿåˆ†æ:")
            for i, (stock_code, sentiment_info) in enumerate(stock_sentiments.items()):
                if i >= 10:
                    break
                logger.info(f"  {stock_code}: {sentiment_info['dominant_sentiment']} "
                           f"(æ–°èæ•¸: {sentiment_info['news_count']}, "
                           f"å¹³å‡åˆ†æ•¸: {sentiment_info['avg_score']:.3f})")
            
            # ä¿å­˜è‚¡ç¥¨æƒ…æ„Ÿåˆ†æçµæœ
            stock_sentiment_df = pd.DataFrame([
                {
                    'stock_code': stock_code,
                    'news_count': info['news_count'],
                    'positive_count': info['sentiment_distribution'].get('positive', 0),
                    'negative_count': info['sentiment_distribution'].get('negative', 0),
                    'neutral_count': info['sentiment_distribution'].get('neutral', 0),
                    'avg_sentiment_score': info['avg_score'],
                    'avg_confidence': info['avg_confidence'],
                    'dominant_sentiment': info['dominant_sentiment']
                }
                for stock_code, info in stock_sentiments.items()
            ])
            
            stock_output_file = get_data_file_path("processed/stock_sentiments.csv")
            stock_sentiment_df.to_csv(stock_output_file, index=False, encoding='utf-8-sig')
            logger.info(f"è‚¡ç¥¨æƒ…æ„Ÿåˆ†æçµæœå·²ä¿å­˜åˆ°: {stock_output_file}")
        else:
            logger.info("æ²’æœ‰æ‰¾åˆ°åŒ…å«è‚¡ç¥¨ä»£ç¢¼çš„æ–°è")
        
    except Exception as e:
        logger.error(f"åˆ†æè‚¡ç¥¨æƒ…æ„Ÿæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def check_system_status():
    """æª¢æŸ¥ç³»çµ±ç‹€æ…‹å’Œæ•¸æ“šå®Œæ•´æ€§"""
    logger.info("ğŸ” è‚¡ç¥¨é¸æ“‡ç³»çµ±ç‹€æ…‹æª¢æŸ¥")
    
    # æª¢æŸ¥æ•¸æ“šæ–‡ä»¶
    logger.info("\n=== æª¢æŸ¥æ•¸æ“šæ–‡ä»¶ç‹€æ…‹ ===")
    
    data_files = {
        "è‚¡åƒ¹æ•¸æ“š": "data/raw/prices.csv",
        "æ–°èæ•¸æ“š": "data/raw/news.csv", 
        "çµ±ä¸€æ–°è": "data/raw/unified_news.csv",
        "æ–°èæƒ…æ„Ÿåˆ†æ": "data/processed/news_with_sentiment.csv",
        "TPEXæ‰‹å‹•æ•¸æ“š": "data/processed/tpex_manual_data.csv"
    }
    
    for name, path in data_files.items():
        file_path = Path(path)
        if file_path.exists():
            try:
                df = pd.read_csv(file_path)
                logger.info(f"âœ… {name}: {len(df)} ç­†æ•¸æ“š")
            except Exception as e:
                logger.warning(f"âŒ {name}: è®€å–éŒ¯èª¤ - {str(e)[:50]}")
        else:
            logger.warning(f"âŒ {name}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶
    logger.info("\n=== æª¢æŸ¥æ¨¡å‹æ–‡ä»¶ç‹€æ…‹ ===")
    
    models_dir = Path("outputs/models")
    model_files = [
        "feature_columns.pkl",
        "training_metadata.pkl", 
        "logistic_regression_model.pkl",
        "xgboost_classifier_model.pkl",
        "xgboost_regressor_model.pkl"
    ]
    
    for model_file in model_files:
        model_path = models_dir / model_file
        if model_path.exists():
            size = model_path.stat().st_size
            logger.info(f"âœ… {model_file}: {size} å­—ç¯€")
        else:
            logger.warning(f"âŒ {model_file}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æª¢æŸ¥æ ¸å¿ƒè…³æœ¬
    logger.info("\n=== æª¢æŸ¥æ ¸å¿ƒè…³æœ¬æ–‡ä»¶ ===")
    
    scripts = [
        "prepare_data.py",
        "train.py", 
        "predict.py",
        "run_backtest.py",
        "process_news.py",
        "process_manual_tpex.py"
    ]
    
    for script in scripts:
        script_path = Path(script)
        if script_path.exists():
            size = script_path.stat().st_size
            logger.info(f"âœ… {script}: {size} å­—ç¯€")
        else:
            logger.warning(f"âŒ {script}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    logger.info("\n=== ç³»çµ±ç‹€æ…‹æª¢æŸ¥å®Œæˆ ===")


def prepare_all_data(force_refresh=False, pages_per_source=3, include_news=True, use_api=False):
    """æº–å‚™æ‰€æœ‰æ•¸æ“š"""
    logger.info("=== é–‹å§‹æº–å‚™æ‰€æœ‰æ•¸æ“š ===")
    
    # æ­¥é©Ÿ1: ç²å–è‚¡åƒ¹æ•¸æ“š
    logger.info("æ­¥é©Ÿ1: ç²å–è‚¡åƒ¹æ•¸æ“š...")
    price_df = fetch_price_data(force_refresh, use_api)
    if price_df is None:
        logger.error("è‚¡åƒ¹æ•¸æ“šç²å–å¤±æ•—")
        return False
    
    # æ­¥é©Ÿ2: ç²å–æ–°èæ•¸æ“šï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
    if include_news:
        logger.info("æ­¥é©Ÿ2: ç²å–æ–°èæ•¸æ“š...")
        news_df = fetch_news_data(force_refresh, pages_per_source)
        if news_df is not None and not news_df.empty:
            # æ­¥é©Ÿ3: æ–°èæƒ…æ„Ÿåˆ†æ
            logger.info("æ­¥é©Ÿ3: é€²è¡Œæ–°èæƒ…æ„Ÿåˆ†æ...")
            sentiment_df = analyze_news_sentiment(news_df)
            if sentiment_df is not None:
                # æ­¥é©Ÿ4: è‚¡ç¥¨æƒ…æ„Ÿåˆ†æ
                logger.info("æ­¥é©Ÿ4: åˆ†æå„è‚¡ç¥¨çš„æƒ…æ„Ÿ...")
                analyze_stock_sentiment(sentiment_df)
        else:
            logger.warning("æ–°èæ•¸æ“šç²å–å¤±æ•—æˆ–ç‚ºç©º")
    else:
        logger.info("è·³éæ–°èæ•¸æ“šè™•ç†")
    
    logger.info("=== æ•¸æ“šæº–å‚™å®Œæˆ ===")
    return True


def main():
    """ä¸»å‡½æ•¸"""
    parser = argparse.ArgumentParser(description='çµ±ä¸€æ•¸æ“šæº–å‚™è…³æœ¬')
    parser.add_argument('--check', action='store_true', help='æª¢æŸ¥ç³»çµ±ç‹€æ…‹å’Œæ•¸æ“šå®Œæ•´æ€§')
    parser.add_argument('--force', action='store_true', help='å¼·åˆ¶é‡æ–°ç²å–æ‰€æœ‰æ•¸æ“š')
    parser.add_argument('--prices-only', action='store_true', help='åªç²å–è‚¡åƒ¹æ•¸æ“š')
    parser.add_argument('--news-only', action='store_true', help='åªç²å–æ–°èæ•¸æ“š')
    parser.add_argument('--no-news', action='store_true', help='è·³éæ–°èæ•¸æ“šè™•ç†')
    parser.add_argument('--use-api', action='store_true', help='å¼·åˆ¶ä½¿ç”¨APIç²å–è‚¡åƒ¹æ•¸æ“š')
    parser.add_argument('--pages', type=int, default=3, help='æ¯å€‹æ–°èä¾†æºæŠ“å–çš„é æ•¸')
    
    args = parser.parse_args()
    
    logger.info("=== çµ±ä¸€æ•¸æ“šæº–å‚™é–‹å§‹ ===")
    
    if args.check:
        logger.info("=== ç³»çµ±ç‹€æ…‹æª¢æŸ¥ ===")
        check_system_status()
    else:
        if args.prices_only:
            fetch_price_data(args.force, args.use_api)
        elif args.news_only:
            fetch_news_data(args.force, args.pages)
        else:
            # é è¨­ç²å–æ‰€æœ‰æ•¸æ“š
            prepare_all_data(
                force_refresh=args.force,
                pages_per_source=args.pages,
                include_news=not args.no_news,
                use_api=args.use_api
            )
    
    logger.info("=== çµ±ä¸€æ•¸æ“šæº–å‚™çµæŸ ===")


if __name__ == "__main__":
    main()
